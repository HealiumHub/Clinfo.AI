{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please create another jupyter notebook server inside UniEval folder before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aaron.dinh.int/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class EvaluationType(Enum):\n",
    "    ResSearch = \"RestrictedSearch\"\n",
    "    SrcDropped = \"SourcedDropped\"\n",
    "    UnresSearch= \"UnrestrictedSearch\"\n",
    "\n",
    "class ResponseType(Enum):\n",
    "    TLDR = \"TLDR\"\n",
    "    Synthesis = \"Synthesis\"\n",
    "    Combined= \"Combined\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_type: EvaluationType, is_vietnamese: bool = False):\n",
    "    path = \"\"\n",
    "    if is_vietnamese:\n",
    "        path = os.path.join(f\"../VN_{eval_type.value}.csv\")\n",
    "    else:\n",
    "        path = os.path.join(f\"../{eval_type.value}.csv\")\n",
    "    test = pd.read_csv(path)\n",
    "    test.head()\n",
    "\n",
    "    src_list=[]\n",
    "    ref_list=[]\n",
    "    output_TLDR=[]\n",
    "    output_Summary=[]\n",
    "    output_All=[]\n",
    "\n",
    "\n",
    "    for index, row in test.iterrows():\n",
    "        # Example for summarization\n",
    "        task = 'summarization'\n",
    "\n",
    "        # a list of source documents\n",
    "        src_list.append(row['Abstract'])\n",
    "        # a list of human-annotated reference summaries\n",
    "        ref_list.append(row['HumanAnswer'])\n",
    "        \n",
    "        output_TLDR.append(row['TLDR'])\n",
    "        output_Summary.append(row['Summary'])\n",
    "        output_All.append(row['All'])\n",
    "\n",
    "\n",
    "    data_TLDR = convert_to_json(output_list=output_TLDR, \n",
    "                        src_list=src_list, ref_list=ref_list)\n",
    "    \n",
    "    data_Summary = convert_to_json(output_list=output_Summary, \n",
    "                    src_list=src_list, ref_list=ref_list)\n",
    "    data_All = convert_to_json(output_list=output_All, \n",
    "                        src_list=src_list, ref_list=ref_list)\n",
    "    # Initialize evaluator for a specific task\n",
    "    evaluator = get_evaluator(task, device=\"mps\")\n",
    "\n",
    "    # Get multi-dimensional evaluation scores\n",
    "    print(\"Evaluating TLDR\")\n",
    "    eval_TLDR = evaluator.evaluate(data_TLDR, print_result=True)\n",
    "    print(\"------------------------\")\n",
    "    print(\"Evaluating Summary\")\n",
    "    eval_Summary = evaluator.evaluate(data_Summary, print_result=True)\n",
    "    print(\"------------------------\")\n",
    "    print(\"Evaluating All\")\n",
    "    eval_All = evaluator.evaluate(data_All, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron.dinh.int/Library/Caches/pypoetry/virtualenvs/clinfo-ai-vatf6MVj-py3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TLDR\n",
      "Evaluating coherence of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:40<00:00, 50.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:54<00:00, 57.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:56<00:00, 58.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:36<00:00, 78.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.802566 |\n",
      "| consistency | 0.805406 |\n",
      "|   fluency   | 0.95495  |\n",
      "|  relevance  | 0.855385 |\n",
      "|   overall   | 0.854577 |\n",
      "+-------------+----------+\n",
      "------------------------\n",
      "Evaluating Summary\n",
      "Evaluating coherence of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:20<00:00, 70.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [09:09<00:00, 91.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [03:37<00:00, 36.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [20:31<00:00, 615.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.836162 |\n",
      "| consistency | 0.626847 |\n",
      "|   fluency   | 0.915165 |\n",
      "|  relevance  | 0.91747  |\n",
      "|   overall   | 0.823911 |\n",
      "+-------------+----------+\n",
      "------------------------\n",
      "Evaluating All\n",
      "Evaluating coherence of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [04:41<00:00, 140.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [11:18<00:00, 84.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [02:18<00:00, 17.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 11 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:53<00:00, 26.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.921498 |\n",
      "| consistency | 0.65814  |\n",
      "|   fluency   | 0.924644 |\n",
      "|  relevance  | 0.966328 |\n",
      "|   overall   | 0.867652 |\n",
      "+-------------+----------+\n"
     ]
    }
   ],
   "source": [
    "evaluate(EvaluationType.UnresSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron.dinh.int/Library/Caches/pypoetry/virtualenvs/clinfo-ai-vatf6MVj-py3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TLDR\n",
      "Evaluating coherence of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:06<00:00, 63.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [03:18<00:00, 99.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:42<00:00, 81.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:13<00:00, 36.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.538442 |\n",
      "| consistency | 0.536401 |\n",
      "|   fluency   | 0.679237 |\n",
      "|  relevance  | 0.510135 |\n",
      "|   overall   | 0.566054 |\n",
      "+-------------+----------+\n",
      "------------------------\n",
      "Evaluating Summary\n",
      "Evaluating coherence of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:13<00:00, 36.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [08:28<00:00, 127.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [04:56<00:00, 74.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:02<00:00, 61.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.430905 |\n",
      "| consistency | 0.350314 |\n",
      "|   fluency   | 0.573134 |\n",
      "|  relevance  | 0.384803 |\n",
      "|   overall   | 0.434789 |\n",
      "+-------------+----------+\n",
      "------------------------\n",
      "Evaluating All\n",
      "Evaluating coherence of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [08:50<00:00, 265.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [09:01<00:00, 90.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [04:44<00:00, 47.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 10 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:59<00:00, 59.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.570444 |\n",
      "| consistency | 0.408035 |\n",
      "|   fluency   | 0.62881  |\n",
      "|  relevance  | 0.565136 |\n",
      "|   overall   | 0.543106 |\n",
      "+-------------+----------+\n"
     ]
    }
   ],
   "source": [
    "evaluate(EvaluationType.UnresSearch, is_vietnamese=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "import json\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import Settings\n",
    "from pydantic import BaseModel\n",
    "from llama_index.program.openai import OpenAIPydanticProgram\n",
    "load_dotenv()\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# create prompt template\n",
    "qa_prompt_tmpl_str = \"\"\"\\\n",
    "    You are an Vietnamese Medical Expert who is evaluating the quality of a medical summary.\n",
    "    You are given an json object containing the following fields:\n",
    "    - source: A list contains the abstract of the medical paper in Vietnamese\n",
    "    - reference: A list contains the human-annotated reference summaries in Vietnamese\n",
    "    - system_output: A list contains the model-generated summaries in Vietnamese\n",
    "    Please evaluate the quality of the model-generated summaries based on the following criteria:\n",
    "    - Coherence: The summary is logically consistent and flows well\n",
    "    - Consistency: The summary does not contradict itself\n",
    "    - Relevance: The summary is relevant to the source document\n",
    "    - Fluency: The summary is grammatically correct and sounds natural\n",
    "    - Overall: The overall quality of the summary\n",
    "    Please provide a score between 0 and 1 with 2 decimal places for each of the criteria.\n",
    "    --------------------------------------------------\n",
    "    Given the context information and not prior knowledge, evaluate this json: {json_obj}\n",
    "    Please provide a score between 0 and 1 for each of the criteria only in object format.\n",
    "\"\"\"  \n",
    "\n",
    "# qa_prompt_tmpl_str.format(json_obj=json_str)\n",
    "class Evaluation(BaseModel):\n",
    "    coherence: float\n",
    "    consistency: float\n",
    "    relevance: float\n",
    "    fluency: float\n",
    "    overall: float\n",
    "\n",
    "Settings.llm =  OpenAI(model=\"gpt-4o\", api_key=OPEN_AI_API_KEY, context_window=1280000)\n",
    "\n",
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=Evaluation, prompt_template_str=qa_prompt_tmpl_str, verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = evaluate(EvaluationType.UnresSearch, is_vietnamese=True)\n",
    "\n",
    "TLDR = data[\"TLDR\"]\n",
    "Summary = data[\"Summary\"]\n",
    "All = data[\"All\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_eval_scores(data):\n",
    "    output = program(\n",
    "    json_obj=data, description=\"Data model for an Evaluation.\"\n",
    ")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: Evaluation with args: {\"coherence\":0.95,\"consistency\":0.95,\"relevance\":0.9,\"fluency\":0.95,\"overall\":0.94}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluation(coherence=0.95, consistency=0.95, relevance=0.9, fluency=0.95, overall=0.94)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TLDR\n",
    "response = get_eval_scores(TLDR)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: Evaluation with args: {\"coherence\":0.95,\"consistency\":0.95,\"relevance\":0.9,\"fluency\":0.9,\"overall\":0.92}\n",
      "{\"coherence\":0.95,\"consistency\":0.95,\"relevance\":0.9,\"fluency\":0.9,\"overall\":0.92}\n"
     ]
    }
   ],
   "source": [
    "#Synthesis\n",
    "response = get_eval_scores(Summary)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: Evaluation with args: {\"coherence\":0.85,\"consistency\":0.9,\"relevance\":0.88,\"fluency\":0.87,\"overall\":0.88}\n",
      "{\"coherence\":0.85,\"consistency\":0.9,\"relevance\":0.88,\"fluency\":0.87,\"overall\":0.88}\n"
     ]
    }
   ],
   "source": [
    "# Synthesis + TLDR\n",
    "response = get_eval_scores(All)\n",
    "print(response.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
